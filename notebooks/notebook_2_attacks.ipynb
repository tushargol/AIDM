{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack Generation and Demonstration\n",
    "\n",
    "This notebook demonstrates various attack generation techniques for power system security evaluation.\n",
    "\n",
    "## Attack Types:\n",
    "1. FDIA (False Data Injection Attack)\n",
    "2. Temporal Stealth Attacks\n",
    "3. Replay Attacks\n",
    "4. ML Adversarial Attacks (FGSM, PGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "\n",
    "from attacks import AttackGenerator\n",
    "from pandapower_utils import create_power_system_model\n",
    "from data_loader import load_sample_data\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open('../config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Load sample data\n",
    "dataset_path = config['data']['dataset_path']\n",
    "data = load_sample_data(dataset_path, small_data_mode=True)\n",
    "\n",
    "print(f\"Loaded {len(data)} data modalities\")\n",
    "for modality, df in data.items():\n",
    "    print(f\"  {modality}: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Attack Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize attack generator\n",
    "generator = AttackGenerator(config)\n",
    "generator.initialize_power_model()\n",
    "\n",
    "# Generate base measurements for attacks\n",
    "if generator.power_model:\n",
    "    measurements, timestamps = generator.power_model.generate_base_measurements(n_samples=200)\n",
    "    print(f\"Generated measurements: {measurements.shape}\")\n",
    "else:\n",
    "    # Fallback synthetic measurements\n",
    "    measurements = np.random.randn(200, 15) * 0.1 + 1.0\n",
    "    timestamps = pd.date_range('2024-01-01', periods=200, freq='1S')\n",
    "    print(f\"Using synthetic measurements: {measurements.shape}\")\n",
    "\n",
    "print(f\"Jacobian available: {generator.jacobian is not None}\")\n",
    "if generator.jacobian is not None:\n",
    "    print(f\"Jacobian shape: {generator.jacobian.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FDIA (False Data Injection Attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate FDIA\n",
    "print(\"Generating FDIA...\")\n",
    "\n",
    "# Select a measurement sample\n",
    "z_clean = measurements[50]\n",
    "print(f\"Original measurement: {z_clean[:5]}\")\n",
    "\n",
    "# Generate attack with different magnitudes\n",
    "attack_magnitudes = [0.05, 0.1, 0.2]\n",
    "fdia_results = {}\n",
    "\n",
    "for mag in attack_magnitudes:\n",
    "    z_attacked = generator.make_fdia(z_clean, attack_magnitude=mag)\n",
    "    attack_vector = z_attacked - z_clean\n",
    "    fdia_results[mag] = {\n",
    "        'attacked': z_attacked,\n",
    "        'attack_vector': attack_vector,\n",
    "        'attack_norm': np.linalg.norm(attack_vector)\n",
    "    }\n",
    "    print(f\"Attack magnitude {mag}: norm = {fdia_results[mag]['attack_norm']:.4f}\")\n",
    "\n",
    "# Visualize FDIA\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot original vs attacked measurements\n",
    "indices = np.arange(len(z_clean))\n",
    "axes[0].bar(indices - 0.2, z_clean, 0.4, label='Original', alpha=0.7)\n",
    "axes[0].bar(indices + 0.2, fdia_results[0.1]['attacked'], 0.4, label='Attacked', alpha=0.7)\n",
    "axes[0].set_title('FDIA: Original vs Attacked Measurements')\n",
    "axes[0].set_xlabel('Measurement Index')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot attack vectors\n",
    "for mag in attack_magnitudes:\n",
    "    axes[1].plot(fdia_results[mag]['attack_vector'], label=f'Magnitude {mag}', marker='o')\n",
    "axes[1].set_title('FDIA Attack Vectors')\n",
    "axes[1].set_xlabel('Measurement Index')\n",
    "axes[1].set_ylabel('Attack Value')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Temporal Stealth Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate temporal stealth attack\n",
    "print(\"Generating Temporal Stealth Attack...\")\n",
    "\n",
    "# Use a subset of measurements for temporal attack\n",
    "Z_window = measurements[100:150]  # 50 timesteps\n",
    "print(f\"Time window shape: {Z_window.shape}\")\n",
    "\n",
    "# Select measurements to attack (first 3 measurements)\n",
    "attack_indices = [0, 1, 2]\n",
    "\n",
    "# Generate stealth attack\n",
    "Z_stealth = generator.make_temporal_stealth(\n",
    "    Z_window, \n",
    "    indices=attack_indices,\n",
    "    duration=20,\n",
    "    max_step=0.02\n",
    ")\n",
    "\n",
    "# Visualize temporal stealth attack\n",
    "fig, axes = plt.subplots(len(attack_indices), 1, figsize=(12, 8))\n",
    "\n",
    "for i, idx in enumerate(attack_indices):\n",
    "    ax = axes[i] if len(attack_indices) > 1 else axes\n",
    "    \n",
    "    time_indices = np.arange(len(Z_window))\n",
    "    ax.plot(time_indices, Z_window[:, idx], label='Original', linewidth=2)\n",
    "    ax.plot(time_indices, Z_stealth[:, idx], label='Stealth Attack', linewidth=2, linestyle='--')\n",
    "    \n",
    "    ax.set_title(f'Temporal Stealth Attack - Measurement {idx}')\n",
    "    ax.set_xlabel('Time Step')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate attack statistics\n",
    "attack_diff = Z_stealth - Z_window\n",
    "max_attack = np.max(np.abs(attack_diff))\n",
    "mean_attack = np.mean(np.abs(attack_diff))\n",
    "print(f\"\\nStealth Attack Statistics:\")\n",
    "print(f\"  Maximum attack magnitude: {max_attack:.4f}\")\n",
    "print(f\"  Mean attack magnitude: {mean_attack:.4f}\")\n",
    "print(f\"  Attacked measurements: {len(attack_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Replay Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate replay attack\n",
    "print(\"Generating Replay Attack...\")\n",
    "\n",
    "# Use larger measurement window\n",
    "Z_replay_window = measurements[50:150]  # 100 timesteps\n",
    "print(f\"Replay window shape: {Z_replay_window.shape}\")\n",
    "\n",
    "# Generate replay attack\n",
    "source_window = (10, 20)  # Copy timesteps 10-20\n",
    "target_time = 70  # Paste at timestep 70\n",
    "\n",
    "Z_replay = generator.make_replay(\n",
    "    Z_replay_window,\n",
    "    source_window=source_window,\n",
    "    target_time=target_time\n",
    ")\n",
    "\n",
    "# Visualize replay attack\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 8))\n",
    "\n",
    "# Plot first measurement channel\n",
    "time_indices = np.arange(len(Z_replay_window))\n",
    "axes[0].plot(time_indices, Z_replay_window[:, 0], label='Original', linewidth=2)\n",
    "axes[0].plot(time_indices, Z_replay[:, 0], label='With Replay Attack', linewidth=2, linestyle='--')\n",
    "\n",
    "# Highlight source and target regions\n",
    "axes[0].axvspan(source_window[0], source_window[1], alpha=0.3, color='green', label='Source Window')\n",
    "axes[0].axvspan(target_time, target_time + (source_window[1] - source_window[0]), \n",
    "                alpha=0.3, color='red', label='Target Window')\n",
    "\n",
    "axes[0].set_title('Replay Attack - Measurement Channel 0')\n",
    "axes[0].set_xlabel('Time Step')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot difference (attack signal)\n",
    "replay_diff = Z_replay - Z_replay_window\n",
    "axes[1].plot(time_indices, replay_diff[:, 0], color='red', linewidth=2)\n",
    "axes[1].set_title('Replay Attack Signal (Difference)')\n",
    "axes[1].set_xlabel('Time Step')\n",
    "axes[1].set_ylabel('Attack Magnitude')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate replay statistics\n",
    "replay_affected_samples = np.sum(np.any(replay_diff != 0, axis=1))\n",
    "print(f\"\\nReplay Attack Statistics:\")\n",
    "print(f\"  Source window: timesteps {source_window[0]}-{source_window[1]}\")\n",
    "print(f\"  Target window: timesteps {target_time}-{target_time + (source_window[1] - source_window[0])}\")\n",
    "print(f\"  Affected timesteps: {replay_affected_samples}\")\n",
    "print(f\"  Window size: {source_window[1] - source_window[0]} timesteps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ML Adversarial Attacks (ART Integration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ML adversarial attacks using ART\n",
    "print(\"Generating ML Adversarial Attacks...\")\n",
    "\n",
    "try:\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    \n",
    "    # Create a simple classifier for demonstration\n",
    "    X_demo = measurements[:100]\n",
    "    y_demo = np.random.choice([0, 1], 100, p=[0.8, 0.2])  # Synthetic labels\n",
    "    \n",
    "    # Train simple RF classifier\n",
    "    rf_model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "    rf_model.fit(X_demo, y_demo)\n",
    "    \n",
    "    print(f\"Trained RF classifier on {len(X_demo)} samples\")\n",
    "    \n",
    "    # Generate adversarial examples\n",
    "    X_test = measurements[100:120]  # 20 test samples\n",
    "    \n",
    "    adversarial_examples = generator.make_art_attacks(\n",
    "        rf_model, \n",
    "        X_test,\n",
    "        attack_params=config['attacks']['art_attacks']\n",
    "    )\n",
    "    \n",
    "    if adversarial_examples:\n",
    "        print(f\"\\nGenerated adversarial examples:\")\n",
    "        for attack_name, X_adv in adversarial_examples.items():\n",
    "            # Calculate perturbation statistics\n",
    "            perturbation = X_adv - X_test\n",
    "            l2_norm = np.mean(np.linalg.norm(perturbation, axis=1))\n",
    "            linf_norm = np.mean(np.max(np.abs(perturbation), axis=1))\n",
    "            \n",
    "            print(f\"  {attack_name}:\")\n",
    "            print(f\"    L2 norm: {l2_norm:.4f}\")\n",
    "            print(f\"    L∞ norm: {linf_norm:.4f}\")\n",
    "            \n",
    "            # Check if attack changed predictions\n",
    "            pred_clean = rf_model.predict(X_test)\n",
    "            pred_adv = rf_model.predict(X_adv)\n",
    "            success_rate = np.mean(pred_clean != pred_adv)\n",
    "            print(f\"    Success rate: {success_rate:.2%}\")\n",
    "        \n",
    "        # Visualize adversarial examples\n",
    "        if len(adversarial_examples) > 0:\n",
    "            attack_name = list(adversarial_examples.keys())[0]\n",
    "            X_adv_demo = adversarial_examples[attack_name]\n",
    "            \n",
    "            fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "            \n",
    "            # Plot original vs adversarial\n",
    "            sample_idx = 0\n",
    "            feature_indices = np.arange(min(10, X_test.shape[1]))\n",
    "            \n",
    "            axes[0].bar(feature_indices - 0.2, X_test[sample_idx, :len(feature_indices)], \n",
    "                       0.4, label='Original', alpha=0.7)\n",
    "            axes[0].bar(feature_indices + 0.2, X_adv_demo[sample_idx, :len(feature_indices)], \n",
    "                       0.4, label='Adversarial', alpha=0.7)\n",
    "            axes[0].set_title(f'Adversarial Example: {attack_name}')\n",
    "            axes[0].set_xlabel('Feature Index')\n",
    "            axes[0].set_ylabel('Value')\n",
    "            axes[0].legend()\n",
    "            \n",
    "            # Plot perturbation\n",
    "            perturbation_demo = X_adv_demo[sample_idx] - X_test[sample_idx]\n",
    "            axes[1].bar(feature_indices, perturbation_demo[:len(feature_indices)], \n",
    "                       color='red', alpha=0.7)\n",
    "            axes[1].set_title('Adversarial Perturbation')\n",
    "            axes[1].set_xlabel('Feature Index')\n",
    "            axes[1].set_ylabel('Perturbation')\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"No adversarial examples generated (ART may not be available)\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"ART not available: {e}\")\n",
    "    print(\"Skipping ML adversarial attacks demonstration\")\n",
    "except Exception as e:\n",
    "    print(f\"Error generating adversarial examples: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Comprehensive Attack Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive attack dataset\n",
    "print(\"Generating comprehensive attack dataset...\")\n",
    "\n",
    "attack_data = generator.generate_attack_dataset(\n",
    "    measurements=measurements,\n",
    "    timestamps=timestamps,\n",
    "    attack_types=['fdia', 'temporal_stealth', 'replay'],\n",
    "    attack_ratio=0.3\n",
    ")\n",
    "\n",
    "print(f\"\\nAttack Dataset Summary:\")\n",
    "print(f\"  Total samples: {attack_data['metadata']['total_samples']}\")\n",
    "print(f\"  Attack samples: {attack_data['metadata']['attack_samples']}\")\n",
    "print(f\"  Attack ratio: {attack_data['metadata']['attack_ratio']:.2%}\")\n",
    "\n",
    "# Analyze attack distribution\n",
    "attack_types_count = pd.Series(attack_data['attack_types']).value_counts()\n",
    "print(f\"\\nAttack Type Distribution:\")\n",
    "for attack_type, count in attack_types_count.items():\n",
    "    print(f\"  {attack_type}: {count} samples\")\n",
    "\n",
    "# Visualize attack dataset\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Attack labels over time\n",
    "axes[0, 0].plot(attack_data['labels'], linewidth=1)\n",
    "axes[0, 0].set_title('Attack Labels Over Time')\n",
    "axes[0, 0].set_xlabel('Time Step')\n",
    "axes[0, 0].set_ylabel('Attack Label (0=Clean, 1=Attack)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Attack type distribution\n",
    "attack_types_count.plot(kind='bar', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Attack Type Distribution')\n",
    "axes[0, 1].set_xlabel('Attack Type')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 3: Sample measurements (clean vs attacked)\n",
    "clean_indices = np.where(attack_data['labels'] == 0)[0][:50]\n",
    "attack_indices = np.where(attack_data['labels'] == 1)[0][:50]\n",
    "\n",
    "axes[1, 0].plot(attack_data['clean'][clean_indices, 0], label='Clean', alpha=0.7)\n",
    "axes[1, 0].plot(attack_data['clean'][attack_indices, 0], label='Attacked', alpha=0.7)\n",
    "axes[1, 0].set_title('Sample Measurements: Clean vs Attacked')\n",
    "axes[1, 0].set_xlabel('Sample Index')\n",
    "axes[1, 0].set_ylabel('Measurement Value')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Attack magnitude distribution\n",
    "if len(attack_indices) > 0:\n",
    "    attack_magnitudes = np.linalg.norm(\n",
    "        attack_data['clean'][attack_indices] - measurements[attack_indices], axis=1\n",
    "    )\n",
    "    axes[1, 1].hist(attack_magnitudes, bins=20, alpha=0.7, edgecolor='black')\n",
    "    axes[1, 1].set_title('Attack Magnitude Distribution')\n",
    "    axes[1, 1].set_xlabel('L2 Norm of Attack Vector')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save attack dataset\n",
    "generator.save_attack_dataset(\n",
    "    attack_data, \n",
    "    config['data']['output_path'], \n",
    "    'notebook_demo_attacks'\n",
    ")\n",
    "\n",
    "print(f\"\\nAttack dataset saved to: {config['data']['output_path']}/experiments/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ATTACK GENERATION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"Generated Attack Types:\")\n",
    "print(\"  ✓ FDIA (False Data Injection Attack)\")\n",
    "print(\"  ✓ Temporal Stealth Attack\")\n",
    "print(\"  ✓ Replay Attack\")\n",
    "print(\"  ✓ ML Adversarial Attacks (if ART available)\")\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"  Total samples: {len(measurements)}\")\n",
    "print(f\"  Attack samples: {attack_data['metadata']['attack_samples']}\")\n",
    "print(f\"  Attack ratio: {attack_data['metadata']['attack_ratio']:.2%}\")\n",
    "\n",
    "print(f\"\\nKey Findings:\")\n",
    "print(f\"  • FDIA attacks successfully generated using power system Jacobian\")\n",
    "print(f\"  • Temporal stealth attacks show gradual attack progression\")\n",
    "print(f\"  • Replay attacks demonstrate data copying between time windows\")\n",
    "print(f\"  • ML adversarial attacks test classifier robustness\")\n",
    "\n",
    "print(f\"\\nNext Steps:\")\n",
    "print(f\"  1. Use generated attacks to evaluate AIDM detection performance\")\n",
    "print(f\"  2. Run notebook_3_train_evaluate.ipynb for model training\")\n",
    "print(f\"  3. Analyze detection rates for different attack types\")\n",
    "print(f\"  4. Optimize detection thresholds based on attack characteristics\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ATTACK GENERATION COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
