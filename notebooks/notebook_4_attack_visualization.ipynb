{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIDM Attack Visualization and Analysis\n",
    "\n",
    "This notebook provides comprehensive visualization and analysis tools for adversarial attacks generated by the AIDM framework.\n",
    "\n",
    "## Features:\n",
    "- Interactive attack data visualization\n",
    "- Time series analysis of clean vs attacked measurements\n",
    "- Attack detection performance analysis\n",
    "- Dimensionality reduction visualization\n",
    "- Real-time attack generation and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode(connected=True)\n",
    "\n",
    "# Import AIDM modules\n",
    "from visualize_attacks import AttackVisualizer, create_attack_visualizer\n",
    "from attacks import AttackGenerator\n",
    "from data_loader import load_sample_data\n",
    "import yaml\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open('../config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Create visualizer\n",
    "visualizer = create_attack_visualizer(output_dir='../outputs/reports')\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"- Small data mode: {config['compute']['small_data_mode']}\")\n",
    "print(f\"- Output path: {config['data']['output_path']}\")\n",
    "print(f\"- Attack types: {config['attacks'].keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Sample Attack Data\n",
    "\n",
    "First, let's generate some sample attack data for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample attack data\n",
    "generator = AttackGenerator(config)\n",
    "generator.initialize_power_model()\n",
    "\n",
    "# Create synthetic measurements\n",
    "n_samples = 1000\n",
    "n_measurements = 12\n",
    "timestamps = pd.date_range(start='2024-01-01', periods=n_samples, freq='1S')\n",
    "\n",
    "# Generate base measurements (voltage-like)\n",
    "base_measurements = np.random.randn(n_samples, n_measurements) * 0.05 + 1.0\n",
    "base_measurements = np.abs(base_measurements)  # Ensure positive voltages\n",
    "\n",
    "# Generate comprehensive attack dataset\n",
    "attack_data = generator.generate_attack_dataset(\n",
    "    measurements=base_measurements,\n",
    "    timestamps=timestamps,\n",
    "    attack_types=['fdia', 'temporal_stealth', 'replay'],\n",
    "    attack_ratio=0.3\n",
    ")\n",
    "\n",
    "print(f\"Generated attack dataset:\")\n",
    "print(f\"- Total samples: {attack_data['metadata']['total_samples']}\")\n",
    "print(f\"- Attack samples: {attack_data['metadata']['attack_samples']}\")\n",
    "print(f\"- Attack ratio: {attack_data['metadata']['attack_ratio']:.2%}\")\n",
    "print(f\"- Attack types: {attack_data['metadata']['attack_types_used']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Attack Overview Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attack overview plot\n",
    "overview_fig = visualizer.plot_attack_overview(attack_data)\n",
    "overview_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time series showing attacks\n",
    "timeseries_fig = visualizer.plot_attack_timeseries(\n",
    "    attack_data, \n",
    "    measurement_indices=[0, 1, 2],  # First 3 measurements\n",
    "    time_window=(0, 500)  # First 500 samples\n",
    ")\n",
    "timeseries_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Attack Detection Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze detection performance\n",
    "detection_fig = visualizer.plot_attack_detection_analysis(attack_data)\n",
    "detection_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dimensionality Reduction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attacks in reduced dimensional space\n",
    "dimensionality_fig = visualizer.plot_attack_dimensionality_analysis(attack_data)\n",
    "dimensionality_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interactive Dashboard\n",
    "\n",
    "Create a comprehensive interactive dashboard combining all visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive dashboard\n",
    "dashboard_path = visualizer.create_interactive_dashboard(\n",
    "    attack_data,\n",
    "    save_path='../outputs/reports/attack_dashboard.html'\n",
    ")\n",
    "\n",
    "print(f\"üìä Interactive dashboard created: {dashboard_path}\")\n",
    "print(\"Open this file in your web browser for full interactive experience.\")\n",
    "\n",
    "# Display link\n",
    "from IPython.display import HTML\n",
    "HTML(f'<a href=\"{dashboard_path}\" target=\"_blank\">üîó Open Dashboard</a>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Custom Attack Analysis\n",
    "\n",
    "Analyze specific attack types in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze FDIA attacks specifically\n",
    "measurements = attack_data['clean']\n",
    "labels = attack_data['labels']\n",
    "attack_types = attack_data['attack_types']\n",
    "\n",
    "# Extract FDIA attacks\n",
    "fdia_mask = (labels == 1) & (attack_types == 'fdia')\n",
    "fdia_measurements = measurements[fdia_mask]\n",
    "clean_measurements = measurements[labels == 0]\n",
    "\n",
    "if len(fdia_measurements) > 0:\n",
    "    # Compare distributions\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Measurement distributions\n",
    "    axes[0,0].hist(clean_measurements[:, 0], alpha=0.7, label='Clean', bins=30)\n",
    "    axes[0,0].hist(fdia_measurements[:, 0], alpha=0.7, label='FDIA', bins=30)\n",
    "    axes[0,0].set_title('Measurement 0 Distribution')\n",
    "    axes[0,0].legend()\n",
    "    \n",
    "    # Attack magnitude distribution\n",
    "    if len(fdia_measurements) <= len(clean_measurements):\n",
    "        attack_magnitudes = np.linalg.norm(\n",
    "            fdia_measurements - clean_measurements[:len(fdia_measurements)], axis=1\n",
    "        )\n",
    "        axes[0,1].hist(attack_magnitudes, bins=20, alpha=0.7, color='red')\n",
    "        axes[0,1].set_title('FDIA Attack Magnitudes')\n",
    "        axes[0,1].set_xlabel('L2 Norm of Attack Vector')\n",
    "    \n",
    "    # Correlation analysis\n",
    "    clean_corr = np.corrcoef(clean_measurements[:100].T)\n",
    "    im1 = axes[1,0].imshow(clean_corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    axes[1,0].set_title('Clean Data Correlation')\n",
    "    plt.colorbar(im1, ax=axes[1,0])\n",
    "    \n",
    "    if len(fdia_measurements) >= 10:\n",
    "        fdia_corr = np.corrcoef(fdia_measurements[:min(100, len(fdia_measurements))].T)\n",
    "        im2 = axes[1,1].imshow(fdia_corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "        axes[1,1].set_title('FDIA Data Correlation')\n",
    "        plt.colorbar(im2, ax=axes[1,1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"FDIA Analysis:\")\n",
    "    print(f\"- Number of FDIA attacks: {len(fdia_measurements)}\")\n",
    "    if len(attack_magnitudes) > 0:\n",
    "        print(f\"- Mean attack magnitude: {np.mean(attack_magnitudes):.4f}\")\n",
    "        print(f\"- Max attack magnitude: {np.max(attack_magnitudes):.4f}\")\nelse:\n    print(\"No FDIA attacks found in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Real-time Attack Simulation\n",
    "\n",
    "Simulate and visualize attacks in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-time attack simulation\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def simulate_realtime_attacks(duration_seconds=30, update_interval=2):\n",
    "    \"\"\"Simulate real-time attack detection.\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Initialize data buffers\n",
    "    window_size = 50\n",
    "    time_buffer = []\n",
    "    measurement_buffer = []\n",
    "    attack_buffer = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    while time.time() - start_time < duration_seconds:\n",
    "        # Generate new measurement\n",
    "        current_time = time.time() - start_time\n",
    "        \n",
    "        # Base measurement with some variation\n",
    "        base_value = 1.0 + 0.1 * np.sin(current_time * 0.5)\n",
    "        noise = np.random.normal(0, 0.02)\n",
    "        measurement = base_value + noise\n",
    "        \n",
    "        # Randomly inject attacks\n",
    "        is_attack = np.random.random() < 0.2  # 20% attack probability\n",
    "        if is_attack:\n",
    "            attack_magnitude = np.random.uniform(0.1, 0.3)\n",
    "            measurement += attack_magnitude\n",
    "        \n",
    "        # Update buffers\n",
    "        time_buffer.append(current_time)\n",
    "        measurement_buffer.append(measurement)\n",
    "        attack_buffer.append(is_attack)\n",
    "        \n",
    "        # Keep only recent data\n",
    "        if len(time_buffer) > window_size:\n",
    "            time_buffer.pop(0)\n",
    "            measurement_buffer.pop(0)\n",
    "            attack_buffer.pop(0)\n",
    "        \n",
    "        # Update plots every few iterations\n",
    "        if len(time_buffer) % 5 == 0:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            # Plot 1: Time series\n",
    "            ax1.clear()\n",
    "            colors = ['green' if not att else 'red' for att in attack_buffer]\n",
    "            ax1.scatter(time_buffer, measurement_buffer, c=colors, alpha=0.7)\n",
    "            ax1.plot(time_buffer, measurement_buffer, 'b-', alpha=0.3)\n",
    "            ax1.set_title('Real-time Measurement Stream')\n",
    "            ax1.set_ylabel('Measurement Value')\n",
    "            ax1.grid(True)\n",
    "            \n",
    "            # Plot 2: Attack detection rate\n",
    "            ax2.clear()\n",
    "            if len(attack_buffer) >= 10:\n",
    "                attack_rate = np.convolve(attack_buffer, np.ones(10)/10, mode='valid')\n",
    "                ax2.plot(time_buffer[9:], attack_rate, 'r-', linewidth=2)\n",
    "                ax2.set_title('Attack Detection Rate (10-sample window)')\n",
    "                ax2.set_ylabel('Attack Rate')\n",
    "                ax2.set_xlabel('Time (seconds)')\n",
    "                ax2.set_ylim(0, 1)\n",
    "                ax2.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        time.sleep(update_interval / 10)  # Small delay\n",
    "    \n",
    "    print(\"\\n‚úÖ Real-time simulation completed!\")\n",
    "    total_attacks = sum(attack_buffer)\n",
    "    print(f\"Total attacks detected: {total_attacks}/{len(attack_buffer)} ({total_attacks/len(attack_buffer)*100:.1f}%)\")\n",
    "\n",
    "# Run simulation (uncomment to run)\n",
    "# simulate_realtime_attacks(duration_seconds=20)\n",
    "print(\"üí° Uncomment the line above to run real-time attack simulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Results\n",
    "\n",
    "Save all visualizations and analysis results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save attack data for future use\n",
    "output_dir = '../outputs/experiments'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save attack dataset\n",
    "generator.save_attack_dataset(\n",
    "    attack_data, \n",
    "    '../outputs', \n",
    "    'notebook_demo_attacks'\n",
    ")\n",
    "\n",
    "# Save individual plots\n",
    "plot_dir = '../outputs/reports/individual_plots'\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "overview_fig.write_html(f'{plot_dir}/attack_overview.html')\n",
    "timeseries_fig.write_html(f'{plot_dir}/attack_timeseries.html')\n",
    "detection_fig.write_html(f'{plot_dir}/attack_detection.html')\n",
    "dimensionality_fig.write_html(f'{plot_dir}/attack_dimensionality.html')\n",
    "\n",
    "print(\"üìÅ Results saved:\")\n",
    "print(f\"- Attack dataset: ../outputs/experiments/notebook_demo_attacks_attacks.npz\")\n",
    "print(f\"- Individual plots: {plot_dir}/\")\n",
    "print(f\"- Interactive dashboard: ../outputs/reports/attack_dashboard.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Attack Data Generation**: Created synthetic attack datasets with multiple attack types\n",
    "2. **Comprehensive Visualization**: Multiple interactive plots showing attack patterns\n",
    "3. **Detection Analysis**: ROC curves and detection performance metrics\n",
    "4. **Dimensionality Analysis**: PCA and t-SNE visualization of attack clusters\n",
    "5. **Real-time Simulation**: Live attack detection simulation\n",
    "6. **Interactive Dashboard**: Combined all visualizations into a single dashboard\n",
    "\n",
    "### Next Steps:\n",
    "- Use real data from the digital-twin-dataset API\n",
    "- Train actual ML models for detection\n",
    "- Implement advanced attack types\n",
    "- Develop real-time detection systems\n",
    "\n",
    "### Key Files Generated:\n",
    "- `attack_dashboard.html`: Interactive dashboard\n",
    "- `notebook_demo_attacks_attacks.npz`: Attack dataset\n",
    "- Individual plot HTML files for sharing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
