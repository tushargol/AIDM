{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration - Digital Twin Power System Data\n",
    "\n",
    "This notebook explores the digital twin dataset structure and visualizes sample data.\n",
    "\n",
    "## Objectives:\n",
    "1. Inspect repository structure and available files\n",
    "2. Load sample data from different modalities\n",
    "3. Visualize voltage magnitudes, phasor angles, and topology data\n",
    "4. Assess data quality and characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from data_loader import DigitalTwinDataLoader, load_sample_data\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Repository Structure Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore dataset directory structure\n",
    "dataset_path = \"../digital-twin-dataset/digital-twin-dataset\"\n",
    "\n",
    "def print_directory_tree(path, prefix=\"\", max_depth=3, current_depth=0):\n",
    "    \"\"\"Print directory tree structure.\"\"\"\n",
    "    if current_depth > max_depth:\n",
    "        return\n",
    "    \n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        print(f\"Path does not exist: {path}\")\n",
    "        return\n",
    "    \n",
    "    items = sorted(path.iterdir())\n",
    "    for i, item in enumerate(items):\n",
    "        is_last = i == len(items) - 1\n",
    "        current_prefix = \"└── \" if is_last else \"├── \"\n",
    "        \n",
    "        if item.is_dir():\n",
    "            print(f\"{prefix}{current_prefix}{item.name}/\")\n",
    "            next_prefix = prefix + (\"    \" if is_last else \"│   \")\n",
    "            print_directory_tree(item, next_prefix, max_depth, current_depth + 1)\n",
    "        else:\n",
    "            size_mb = item.stat().st_size / (1024 * 1024)\n",
    "            print(f\"{prefix}{current_prefix}{item.name} ({size_mb:.2f} MB)\")\n",
    "\n",
    "print(\"Digital Twin Dataset Structure:\")\n",
    "print_directory_tree(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Inspect Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "loader = DigitalTwinDataLoader(dataset_path)\n",
    "\n",
    "# List available files\n",
    "available_files = loader.list_available_files()\n",
    "\n",
    "print(\"Available data files:\")\n",
    "for modality, files in available_files.items():\n",
    "    print(f\"\\n{modality.upper()}:\")\n",
    "    if files:\n",
    "        for file in files[:5]:  # Show first 5 files\n",
    "            print(f\"  - {file}\")\n",
    "        if len(files) > 5:\n",
    "            print(f\"  ... and {len(files) - 5} more files\")\n",
    "    else:\n",
    "        print(\"  No files found (will use synthetic data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data\n",
    "print(\"Loading sample data...\")\n",
    "data = load_sample_data(dataset_path, small_data_mode=True, synthetic_fallback=True)\n",
    "\n",
    "print(\"\\nLoaded data summary:\")\n",
    "for modality, df in data.items():\n",
    "    print(f\"\\n{modality.upper()}:\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Time range: {df.index[0]} to {df.index[-1]}\")\n",
    "    print(f\"  Duration: {df.index[-1] - df.index[0]}\")\n",
    "    print(f\"  Columns: {list(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}\")\n",
    "    print(f\"  Data types: {df.dtypes.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess data quality\n",
    "print(\"Data Quality Assessment:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for modality, df in data.items():\n",
    "    print(f\"\\n{modality.upper()} QUALITY:\")\n",
    "    \n",
    "    # Missing values\n",
    "    missing_count = df.isnull().sum().sum()\n",
    "    missing_pct = (missing_count / df.size) * 100\n",
    "    print(f\"  Missing values: {missing_count} ({missing_pct:.2f}%)\")\n",
    "    \n",
    "    # Numeric columns statistics\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"  Numeric columns: {len(numeric_cols)}\")\n",
    "        print(f\"  Mean range: [{df[numeric_cols].mean().min():.3f}, {df[numeric_cols].mean().max():.3f}]\")\n",
    "        print(f\"  Std range: [{df[numeric_cols].std().min():.3f}, {df[numeric_cols].std().max():.3f}]\")\n",
    "        \n",
    "        # Check for constant columns\n",
    "        constant_cols = df[numeric_cols].columns[df[numeric_cols].std() == 0]\n",
    "        if len(constant_cols) > 0:\n",
    "            print(f\"  Constant columns: {len(constant_cols)}\")\n",
    "    \n",
    "    # Time series regularity\n",
    "    time_diffs = df.index.to_series().diff().dropna()\n",
    "    if len(time_diffs) > 0:\n",
    "        mode_diff = time_diffs.mode()[0] if len(time_diffs.mode()) > 0 else time_diffs.median()\n",
    "        irregular_count = (time_diffs != mode_diff).sum()\n",
    "        print(f\"  Time regularity: {len(time_diffs) - irregular_count}/{len(time_diffs)} regular intervals\")\n",
    "        print(f\"  Sampling rate: ~{mode_diff.total_seconds():.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "n_modalities = len(data)\n",
    "fig, axes = plt.subplots(n_modalities, 2, figsize=(15, 5*n_modalities))\n",
    "if n_modalities == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for i, (modality, df) in enumerate(data.items()):\n",
    "    # Time series plot\n",
    "    ax1 = axes[i, 0]\n",
    "    cols_to_plot = df.columns[:min(5, len(df.columns))]\n",
    "    for col in cols_to_plot:\n",
    "        ax1.plot(df.index, df[col], label=col, alpha=0.7, linewidth=1)\n",
    "    \n",
    "    ax1.set_title(f'{modality.capitalize()} - Time Series')\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel('Value')\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distribution plot\n",
    "    ax2 = axes[i, 1]\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        # Plot histograms for first few numeric columns\n",
    "        cols_to_hist = numeric_cols[:min(3, len(numeric_cols))]\n",
    "        for col in cols_to_hist:\n",
    "            ax2.hist(df[col].dropna(), bins=30, alpha=0.6, label=col, density=True)\n",
    "        \n",
    "        ax2.set_title(f'{modality.capitalize()} - Distributions')\n",
    "        ax2.set_xlabel('Value')\n",
    "        ax2.set_ylabel('Density')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, 'No numeric data\\nfor distribution plot', \n",
    "                ha='center', va='center', transform=ax2.transAxes)\n",
    "        ax2.set_title(f'{modality.capitalize()} - No Numeric Data')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Power System Specific Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze voltage-like measurements (assuming magnitude data represents voltages)\n",
    "if 'magnitude' in data:\n",
    "    magnitude_data = data['magnitude']\n",
    "    \n",
    "    print(\"Voltage Magnitude Analysis:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Voltage statistics\n",
    "    voltage_cols = [col for col in magnitude_data.columns if 'voltage' in col.lower()]\n",
    "    if not voltage_cols:\n",
    "        voltage_cols = magnitude_data.select_dtypes(include=[np.number]).columns[:5]\n",
    "    \n",
    "    if len(voltage_cols) > 0:\n",
    "        voltage_stats = magnitude_data[voltage_cols].describe()\n",
    "        print(f\"Voltage-like measurements statistics:\")\n",
    "        print(voltage_stats)\n",
    "        \n",
    "        # Check for voltage violations (assuming per-unit system)\n",
    "        low_voltage_threshold = 0.95\n",
    "        high_voltage_threshold = 1.05\n",
    "        \n",
    "        violations = {\n",
    "            'low': (magnitude_data[voltage_cols] < low_voltage_threshold).sum().sum(),\n",
    "            'high': (magnitude_data[voltage_cols] > high_voltage_threshold).sum().sum()\n",
    "        }\n",
    "        \n",
    "        total_measurements = magnitude_data[voltage_cols].size\n",
    "        print(f\"\\nVoltage Violations (assuming p.u. system):\")\n",
    "        print(f\"  Low voltage (<{low_voltage_threshold}): {violations['low']} ({violations['low']/total_measurements*100:.2f}%)\")\n",
    "        print(f\"  High voltage (>{high_voltage_threshold}): {violations['high']} ({violations['high']/total_measurements*100:.2f}%)\")\n",
    "\n",
    "# Analyze phasor data if available\n",
    "if 'phasor' in data:\n",
    "    phasor_data = data['phasor']\n",
    "    \n",
    "    print(\"\\nPhasor Angle Analysis:\")\n",
    "    print(\"=\" * 25)\n",
    "    \n",
    "    angle_cols = [col for col in phasor_data.columns if 'angle' in col.lower()]\n",
    "    if not angle_cols:\n",
    "        angle_cols = phasor_data.select_dtypes(include=[np.number]).columns[:3]\n",
    "    \n",
    "    if len(angle_cols) > 0:\n",
    "        angle_stats = phasor_data[angle_cols].describe()\n",
    "        print(f\"Angle measurements statistics (radians):\")\n",
    "        print(angle_stats)\n",
    "        \n",
    "        # Angle differences (system stability indicator)\n",
    "        if len(angle_cols) > 1:\n",
    "            angle_diffs = phasor_data[angle_cols].diff(axis=1).iloc[:, 1:]\n",
    "            max_angle_diff = angle_diffs.abs().max().max()\n",
    "            print(f\"\\nMaximum angle difference: {max_angle_diff:.3f} radians ({np.degrees(max_angle_diff):.1f} degrees)\")\n",
    "\n",
    "# Analyze topology data\n",
    "if 'topology' in data:\n",
    "    topology_data = data['topology']\n",
    "    \n",
    "    print(\"\\nTopology Analysis:\")\n",
    "    print(\"=\" * 20)\n",
    "    \n",
    "    # Switching events\n",
    "    binary_cols = [col for col in topology_data.columns if topology_data[col].nunique() <= 2]\n",
    "    \n",
    "    if binary_cols:\n",
    "        print(f\"Binary topology features: {len(binary_cols)}\")\n",
    "        \n",
    "        # Count switching events\n",
    "        switching_events = 0\n",
    "        for col in binary_cols:\n",
    "            changes = (topology_data[col].diff() != 0).sum()\n",
    "            switching_events += changes\n",
    "        \n",
    "        print(f\"Total switching events detected: {switching_events}\")\n",
    "        \n",
    "        # Show current topology state\n",
    "        current_state = topology_data[binary_cols].iloc[-1]\n",
    "        print(f\"\\nCurrent topology state:\")\n",
    "        for col, state in current_state.items():\n",
    "            status = \"CLOSED\" if state == 1 else \"OPEN\"\n",
    "            print(f\"  {col}: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "print(\"DATASET EXPLORATION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_samples = sum(len(df) for df in data.values())\n",
    "total_features = sum(len(df.columns) for df in data.values())\n",
    "time_span = max(df.index[-1] for df in data.values()) - min(df.index[0] for df in data.values())\n",
    "\n",
    "print(f\"Dataset Overview:\")\n",
    "print(f\"  Modalities: {len(data)}\")\n",
    "print(f\"  Total samples: {total_samples:,}\")\n",
    "print(f\"  Total features: {total_features}\")\n",
    "print(f\"  Time span: {time_span}\")\n",
    "\n",
    "print(f\"\\nData Quality:\")\n",
    "total_missing = sum(df.isnull().sum().sum() for df in data.values())\n",
    "total_values = sum(df.size for df in data.values())\n",
    "missing_pct = (total_missing / total_values) * 100 if total_values > 0 else 0\n",
    "print(f\"  Missing data: {missing_pct:.2f}%\")\n",
    "print(f\"  Data completeness: {100 - missing_pct:.2f}%\")\n",
    "\n",
    "print(f\"\\nRecommendations for AIDM:\")\n",
    "print(f\"  ✓ Data is suitable for time series analysis\")\n",
    "print(f\"  ✓ Multiple modalities available for fusion\")\n",
    "if missing_pct < 5:\n",
    "    print(f\"  ✓ Low missing data rate - good for ML training\")\n",
    "else:\n",
    "    print(f\"  ⚠ Consider imputation strategies for missing data\")\n",
    "\n",
    "print(f\"  ✓ Proceed with preprocessing pipeline\")\n",
    "print(f\"  ✓ Ready for feature engineering and model training\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(f\"  1. Run notebook_1_preprocess.ipynb for data preprocessing\")\n",
    "print(f\"  2. Generate attack datasets with notebook_2_attacks.ipynb\")\n",
    "print(f\"  3. Train and evaluate AIDM with notebook_3_train_evaluate.ipynb\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"EXPLORATION COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
