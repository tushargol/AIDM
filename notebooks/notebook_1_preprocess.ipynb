{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIDM Preprocessing Pipeline\n",
    "\n",
    "This notebook implements the data preprocessing pipeline for the AIDM (Anomaly and Intrusion Detection Model) system.\n",
    "\n",
    "## Pipeline Steps:\n",
    "1. Load raw data from the digital twin dataset\n",
    "2. Align and resample multiple modalities to common time base\n",
    "3. Handle missing data\n",
    "4. Extract engineered features\n",
    "5. Create LSTM sequences\n",
    "6. Split data temporally\n",
    "7. Fit scalers and normalize\n",
    "8. Save processed data for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yaml\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Import our custom modules\n",
    "from data_loader import load_sample_data, DigitalTwinDataLoader\n",
    "from preprocess import DataPreprocessor, load_config\n",
    "\n",
    "# Setup plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✅ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config('../config.yaml')\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Dataset path: {config['data']['dataset_path']}\")\n",
    "print(f\"  Small data mode: {config['compute']['small_data_mode']}\")\n",
    "print(f\"  Base sampling rate: {config['preprocessing']['base_sampling_rate']}s\")\n",
    "print(f\"  Sequence window: {config['preprocessing']['sequence_window']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data\n",
    "dataset_path = config['data']['dataset_path']\n",
    "print(f\"Loading data from: {dataset_path}\")\n",
    "\n",
    "# Load data using our data loader\n",
    "data = load_sample_data(\n",
    "    dataset_path, \n",
    "    small_data_mode=config['compute']['small_data_mode'],\n",
    "    synthetic_fallback=True\n",
    ")\n",
    "\n",
    "print(\"\\nLoaded data modalities:\")\n",
    "for modality, df in data.items():\n",
    "    print(f\"  {modality}: {df.shape} - {df.index[0]} to {df.index[-1]}\")\n",
    "    print(f\"    Columns: {list(df.columns[:5])}{'...' if len(df.columns) > 5 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot sample data for each modality\n",
    "fig, axes = plt.subplots(len(data), 1, figsize=(15, 4*len(data)))\n",
    "if len(data) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, (modality, df) in enumerate(data.items()):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot first few columns\n",
    "    cols_to_plot = df.columns[:min(5, len(df.columns))]\n",
    "    for col in cols_to_plot:\n",
    "        ax.plot(df.index, df[col], label=col, alpha=0.7)\n",
    "    \n",
    "    ax.set_title(f'{modality.capitalize()} Data')\n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality assessment\n",
    "print(\"Data Quality Assessment:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for modality, df in data.items():\n",
    "    print(f\"\\n{modality.upper()}:\")\n",
    "    print(f\"  Shape: {df.shape}\")\n",
    "    print(f\"  Time range: {df.index[0]} to {df.index[-1]}\")\n",
    "    print(f\"  Duration: {df.index[-1] - df.index[0]}\")\n",
    "    print(f\"  Missing values: {df.isnull().sum().sum()}\")\n",
    "    print(f\"  Data types: {df.dtypes.value_counts().to_dict()}\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"  Numeric columns: {len(numeric_cols)}\")\n",
    "        print(f\"  Mean values: {df[numeric_cols].mean().mean():.4f}\")\n",
    "        print(f\"  Std deviation: {df[numeric_cols].std().mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Preprocessor and Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the preprocessor\n",
    "preprocessor = DataPreprocessor(config)\n",
    "print(\"✅ Preprocessor initialized\")\n",
    "\n",
    "# Set output path\n",
    "output_path = config['data']['output_path']\n",
    "print(f\"Output path: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Alignment and Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Align and resample data\n",
    "print(\"Step 1: Aligning and resampling data...\")\n",
    "aligned_df = preprocessor.align_and_resample(data)\n",
    "\n",
    "print(f\"Aligned data shape: {aligned_df.shape}\")\n",
    "print(f\"Time range: {aligned_df.index[0]} to {aligned_df.index[-1]}\")\n",
    "print(f\"Columns: {list(aligned_df.columns[:10])}{'...' if len(aligned_df.columns) > 10 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Missing Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Handle missing data\n",
    "print(\"Step 2: Handling missing data...\")\n",
    "print(f\"Missing values before: {aligned_df.isnull().sum().sum()}\")\n",
    "\n",
    "clean_df = preprocessor.handle_missing_data(aligned_df)\n",
    "\n",
    "print(f\"Missing values after: {clean_df.isnull().sum().sum()}\")\n",
    "print(f\"Data shape after cleaning: {clean_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Feature engineering\n",
    "print(\"Step 3: Extracting engineered features...\")\n",
    "print(f\"Input features: {clean_df.shape[1]}\")\n",
    "\n",
    "feature_df = preprocessor.extract_features(clean_df)\n",
    "\n",
    "print(f\"Output features: {feature_df.shape[1]}\")\n",
    "print(f\"Feature names: {preprocessor.feature_names[:10]}{'...' if len(preprocessor.feature_names) > 10 else ''}\")\n",
    "\n",
    "# Show feature statistics\n",
    "print(\"\\nFeature statistics:\")\n",
    "print(feature_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Sequence Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create sequences\n",
    "print(\"Step 4: Creating LSTM sequences...\")\n",
    "X_sequences, y_next, X_tabular = preprocessor.create_sequences(feature_df)\n",
    "\n",
    "print(f\"Sequence data shape: {X_sequences.shape}\")\n",
    "print(f\"Target data shape: {y_next.shape}\")\n",
    "print(f\"Tabular data shape: {X_tabular.shape}\")\n",
    "print(f\"Sequence window: {config['preprocessing']['sequence_window']} timesteps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Split data\n",
    "print(\"Step 5: Splitting data temporally...\")\n",
    "splits = preprocessor.split_data(X_tabular, X_sequences, y_next)\n",
    "\n",
    "print(\"Data splits:\")\n",
    "for split_name in ['train', 'val', 'test']:\n",
    "    X_key = f'X_{split_name}'\n",
    "    print(f\"  {split_name}: {splits[X_key].shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Scaling and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Fit scalers\n",
    "print(\"Step 6: Fitting scalers on training data...\")\n",
    "preprocessor.fit_scalers(splits['X_train'], splits['X_seq_train'])\n",
    "\n",
    "# Step 7: Transform all data\n",
    "print(\"Step 7: Scaling all data...\")\n",
    "for split_name in ['train', 'val', 'test']:\n",
    "    X_tab_key = f'X_{split_name}'\n",
    "    X_seq_key = f'X_seq_{split_name}'\n",
    "    \n",
    "    splits[X_tab_key], splits[X_seq_key] = preprocessor.transform_data(\n",
    "        splits[X_tab_key], splits[X_seq_key]\n",
    "    )\n",
    "\n",
    "print(\"✅ Data scaling completed\")\n",
    "\n",
    "# Show scaling statistics\n",
    "print(\"\\nScaled data statistics (training set):\")\n",
    "print(f\"Tabular features - Mean: {splits['X_train'].mean():.4f}, Std: {splits['X_train'].std():.4f}\")\n",
    "print(f\"Sequence features - Mean: {splits['X_seq_train'].mean():.4f}, Std: {splits['X_seq_train'].std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Save processed data\n",
    "print(\"Step 8: Saving processed data...\")\n",
    "preprocessor.save_processed_data(splits, output_path)\n",
    "\n",
    "# Verify saved files\n",
    "processed_dir = Path(output_path) / \"processed\"\n",
    "saved_files = list(processed_dir.glob(\"*\"))\n",
    "\n",
    "print(\"\\nSaved files:\")\n",
    "for file_path in saved_files:\n",
    "    size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "    print(f\"  {file_path.name}: {size_mb:.2f} MB\")\n",
    "\n",
    "print(f\"\\n✅ All processed data saved to: {processed_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Visualization and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize processed data distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Feature distributions (first few features)\n",
    "n_features_to_plot = min(5, splits['X_train'].shape[1])\n",
    "for i in range(n_features_to_plot):\n",
    "    axes[0, 0].hist(splits['X_train'][:, i], alpha=0.7, bins=30, label=f'Feature {i}')\n",
    "axes[0, 0].set_title('Training Feature Distributions')\n",
    "axes[0, 0].set_xlabel('Scaled Value')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Plot 2: Data split sizes\n",
    "split_sizes = [len(splits['X_train']), len(splits['X_val']), len(splits['X_test'])]\n",
    "split_labels = ['Train', 'Validation', 'Test']\n",
    "axes[0, 1].bar(split_labels, split_sizes, color=['blue', 'orange', 'green'], alpha=0.7)\n",
    "axes[0, 1].set_title('Data Split Sizes')\n",
    "axes[0, 1].set_ylabel('Number of Samples')\n",
    "\n",
    "# Plot 3: Sequence example (first sequence, first few features)\n",
    "seq_example = splits['X_seq_train'][0, :, :min(3, splits['X_seq_train'].shape[2])]\n",
    "for i in range(seq_example.shape[1]):\n",
    "    axes[1, 0].plot(seq_example[:, i], label=f'Feature {i}', marker='o')\n",
    "axes[1, 0].set_title('Example LSTM Sequence')\n",
    "axes[1, 0].set_xlabel('Timestep')\n",
    "axes[1, 0].set_ylabel('Scaled Value')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Plot 4: Feature correlation heatmap (subset)\n",
    "n_corr_features = min(10, splits['X_train'].shape[1])\n",
    "corr_matrix = np.corrcoef(splits['X_train'][:, :n_corr_features].T)\n",
    "im = axes[1, 1].imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "axes[1, 1].set_title('Feature Correlation Matrix')\n",
    "plt.colorbar(im, ax=axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPROCESSING PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n📊 DATA SUMMARY:\")\n",
    "print(f\"  • Original modalities: {len(data)}\")\n",
    "print(f\"  • Final features: {len(preprocessor.feature_names)}\")\n",
    "print(f\"  • Sequence window: {config['preprocessing']['sequence_window']} timesteps\")\n",
    "print(f\"  • Total samples: {len(X_tabular)}\")\n",
    "\n",
    "print(f\"\\n🔄 DATA SPLITS:\")\n",
    "print(f\"  • Training: {len(splits['X_train'])} samples ({len(splits['X_train'])/len(X_tabular)*100:.1f}%)\")\n",
    "print(f\"  • Validation: {len(splits['X_val'])} samples ({len(splits['X_val'])/len(X_tabular)*100:.1f}%)\")\n",
    "print(f\"  • Test: {len(splits['X_test'])} samples ({len(splits['X_test'])/len(X_tabular)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n💾 OUTPUT FILES:\")\n",
    "print(f\"  • Processed data: {processed_dir / 'processed_data.npz'}\")\n",
    "print(f\"  • Scalers: {processed_dir / 'scalers.pkl'}\")\n",
    "print(f\"  • Config: {processed_dir / 'preprocessing_config.yaml'}\")\n",
    "\n",
    "print(f\"\\n🎯 NEXT STEPS:\")\n",
    "print(f\"  1. Run notebook_2_attacks.ipynb to generate adversarial data\")\n",
    "print(f\"  2. Run notebook_3_train_evaluate.ipynb to train AIDM models\")\n",
    "print(f\"  3. Use CLI: python src/attacks.py --type fdia\")\n",
    "print(f\"  4. Use CLI: python src/train_ids.py --model autoencoder\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load and Verify Saved Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify we can load the saved data\n",
    "print(\"Verifying saved data can be loaded...\")\n",
    "\n",
    "# Load processed data\n",
    "loaded_data = np.load(processed_dir / \"processed_data.npz\", allow_pickle=True)\n",
    "loaded_scalers = joblib.load(processed_dir / \"scalers.pkl\")\n",
    "\n",
    "print(\"\\nLoaded data keys:\")\n",
    "for key in loaded_data.files:\n",
    "    if key != 'feature_names':\n",
    "        print(f\"  {key}: {loaded_data[key].shape}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {len(loaded_data[key])} features\")\n",
    "\n",
    "print(\"\\nLoaded scalers:\")\n",
    "for scaler_name, scaler in loaded_scalers.items():\n",
    "    print(f\"  {scaler_name}: {type(scaler).__name__}\")\n",
    "\n",
    "print(\"\\n✅ Data verification completed successfully!\")\n",
    "print(\"The preprocessing pipeline is ready for the next stage.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
